{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf9a821-251f-4f1d-b5e6-5f173b9e9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from category_encoders import OrdinalEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca15dd1-30af-4513-ae71-96c8ee6b2e4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bcdacc-0e24-43e4-81e6-712cf90428e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clusters(df, col, y, n_cluster, merge=False):\n",
    "    '''\n",
    "        function to boil down a colum to n_cluster\n",
    "        \n",
    "        recive:\n",
    "        \n",
    "            df: a data frame \n",
    "            col: col to make cluster\n",
    "            y: the response variable\n",
    "            n_cluster: stamitation of number of cluster\n",
    "            \n",
    "        return:  \n",
    "            a data frame with 'col' droped, and 'col' + '_cluster' column\n",
    "            is added to the data frame\n",
    "\n",
    "        \n",
    "    '''\n",
    "    col_clusters = KMeans(n_clusters=n_cluster, random_state=777)\n",
    "\n",
    "    # 'Neighborhood' and 'MSSubClass' stats\n",
    "    col_stats = df.groupby(col)[y].describe()\n",
    "\n",
    "    # Getting clusters\n",
    "    col_clusters.fit(col_stats)\n",
    "\n",
    "    # preparing DF with cluster lables to merge\n",
    "    new_name = col + '_Cluster'\n",
    "    col_cluster_df = pd.DataFrame( { col: col_stats.index.to_list(),\n",
    "                                     new_name: col_clusters.labels_.tolist()} )\n",
    "    \n",
    "    if merge:\n",
    "        # merging the clusters with the data frame\n",
    "        df = df.merge(col_cluster_df, how='left', on=col)\n",
    "\n",
    "        df[new_name] = df[new_name].astype(str)\n",
    "        \n",
    "        result = df.drop(columns=col)\n",
    "        \n",
    "    else:\n",
    "        result = col_cluster_df\n",
    "    \n",
    "    return result.copy()\n",
    "\n",
    "\n",
    "# Low frequency categories to 'Other'\n",
    "def lower_than(df, col, percentage):\n",
    "    ''' function that will merge low frequency classes into \n",
    "        a single class 'Others'\n",
    "\n",
    "        parameters:\n",
    "            df: a DataFrame\n",
    "            col: column's name to work on\n",
    "            percentage(%): the threshold like 0.1, 0.2\n",
    "\n",
    "        returns:\n",
    "            df: the data frame with col's classes that are lowers\n",
    "                than 'threshold' been repleced with 'Other' category\n",
    "    '''\n",
    "\n",
    "    # calculating the column frequency\n",
    "    col_freq = df[col].value_counts(normalize=True)\n",
    "\n",
    "    # the getting the column threshold \n",
    "    threshold = col_freq.quantile(q= percentage)\n",
    "\n",
    "    # knowing the classes that are below the threshold\n",
    "    less_freq_classes = col_freq[ col_freq <= threshold ]\n",
    "\n",
    "    others = less_freq_classes.index.to_list()\n",
    "\n",
    "    print(others)\n",
    "\n",
    "    df[col] = df[col].replace(others, 'Others')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sort correlation matrix without duplicate\n",
    "def corr_pair(df, cols, threshold = 0.7):\n",
    "    '''\n",
    "    function that calculate the corrolation of features\n",
    "    and filtering just unique pairs \n",
    "    '''\n",
    "\n",
    "    corr = df[cols].corr().abs()\n",
    "\n",
    "    # creating a matrix same size matrix as 'tr'\n",
    "    # [1 1 1]\n",
    "    # [1 1 1]\n",
    "    # [1 1 1]\n",
    "    temp_matrix = np.ones(corr.shape)\n",
    "\n",
    "    # selecting the upper triagle of the matrix\n",
    "    # excluting the diagonal\n",
    "    # [0 1 1]\n",
    "    # [0 0 1]\n",
    "    # [0 0 0]\n",
    "    upper_tri = np.triu(temp_matrix, k=1)\n",
    "\n",
    "    # converting to booleans\n",
    "    # [FALSE TRUE TRUE]\n",
    "    # [FALSE FLASE TRUE]\n",
    "    # [FALSE FALSE FALSE]\n",
    "    uppper_tri_bool = upper_tri.astype( bool )\n",
    "\n",
    "    # filterin the corroletion matrix\n",
    "    upper_corr = corr.where( uppper_tri_bool )\n",
    "\n",
    "    unique_corr_pair = upper_corr.unstack().dropna()\n",
    "    sorted_pairs = unique_corr_pair.sort_values(ascending=False)\n",
    "\n",
    "    return sorted_pairs[ sorted_pairs > threshold ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef4fc7-4204-4d8c-831b-e2e51c0ff471",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52feccc9-d49a-424c-91d7-7a1256354911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(verbose=False):\n",
    "    \n",
    "    d_tr = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/train.csv', index_col='Id')\n",
    "    d_te = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/test.csv', index_col='Id')\n",
    "    \n",
    "    # Utilities has just 2 categories, and one of them\n",
    "    # just appears once so we delete the whole column.\n",
    "    d_tr = d_tr.drop(columns='Utilities')\n",
    "    d_te = d_te.drop(columns='Utilities')\n",
    "    \n",
    "    # Marking Training Set\n",
    "    d_tr['Training'] = True\n",
    "    d_te['Training'] = False  \n",
    "    \n",
    "    if(verbose):\n",
    "        print('d_tr shape:', d_tr.shape)\n",
    "        print('d_te shape:', d_te.shape)\n",
    "        \n",
    "    return (d_tr.copy(), d_te.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94dca3a-3bcd-4417-ba6a-079ff16ee895",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Analazy 'Y'('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a17f4ce-7db5-4633-8cf2-77153160f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_y(df, verbose=False):\n",
    "    \n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Plotting\n",
    "    if(verbose):\n",
    "        fg, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(11,7))\n",
    "        # chequing the distribution of 'y' = 'SalePrice'\n",
    "        \n",
    "        sns.histplot( df['SalePrice'], bins=50, ax= ax1);\n",
    "        ax1.title.set_text('Skewed SalePrice')\n",
    "    \n",
    "        sns.scatterplot(x=df['GrLivArea'], y=df['SalePrice'], ax=ax3);\n",
    "        ax3.title.set_text('Outliers')\n",
    "    \n",
    "    outliers = df[ df['GrLivArea'] >= 4500].index\n",
    "    \n",
    "    # deleting the outliers in 'GrLivArea'\n",
    "    df.drop(outliers, inplace=True)\n",
    "    \n",
    "    # Plotting\n",
    "    if(verbose):\n",
    "        # cheking again\n",
    "        sns.scatterplot(x=df['GrLivArea'], y=df['SalePrice'], ax=ax4);\n",
    "        ax4.title.set_text('NO Outliers')\n",
    "    \n",
    "    # pipeline to scale and do a powerTransforme on y('SalePrice')\n",
    "    fix_y = Pipeline([('scaler', RobustScaler()), ('power', PowerTransformer(method='yeo-johnson'))])\n",
    "    \n",
    "    y = fix_y.fit_transform( df['SalePrice'].values.reshape(-1, 1) )\n",
    "    \n",
    "    # Plotting\n",
    "    if(verbose):\n",
    "        # checking the distribution of 'y' again\n",
    "        sns.histplot(y, bins=50, ax=ax2);\n",
    "        ax2.title.set_text('yeo-johnson transformed \\'y\\'')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # if(verbose):\n",
    "    print('droped index: \\n', outliers)\n",
    "        \n",
    "    df['SalePrice'] = y.copy()\n",
    "    \n",
    "    return df.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4e36f-2405-4d18-9056-5c998eb3e7ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Combining training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165e0f7d-a6db-49df-ae96-26052ea2a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tr_te(df_tr, df_te, verbose=False):\n",
    "    \n",
    "    all_d = pd.concat([d_tr.copy(), d_te.copy()])\n",
    "    \n",
    "    if(verbose):\n",
    "        print('New data shape: ', all_d.shape)\n",
    "        \n",
    "    return all_d.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ade99-7886-4637-89f1-9c7b0dff611d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imputting 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f383d9-7b69-4ad6-a79a-605c0c65c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_nan(all_data):\n",
    "    \n",
    "    # 34 columns with nan\n",
    "    def fillWithNone(df):\n",
    "        ''' nan in 'PoolQC' means 'no pool' \n",
    "            nan in 'MiscFeature' means 'no misc feature'\n",
    "            nan in 'Alley' means 'no alley acces'\n",
    "            nan in 'Fence' means 'no fence'\n",
    "            nan in 'FireplaceQu' means 'no Fireplace'\n",
    "            nan in 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1'\n",
    "                'BsmtFinType1', 'MasVnrType', 'MSSubClass'\n",
    "                'GarageCond' replaced with 'None' too\n",
    "\n",
    "            recive a df\n",
    "        '''\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        columns = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n",
    "                   'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "                   'GarageQual', 'GarageCond', 'BsmtQual',\n",
    "                   'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "                   'BsmtFinType2', 'MasVnrType'\n",
    "                  ]\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = df[col].fillna('None')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fillWithZero(df):\n",
    "        ''' nan \n",
    "\n",
    "        '''\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        columns = ['GarageYrBlt', 'GarageArea', 'GarageCars',\n",
    "                   'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                   'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n",
    "                   'MasVnrArea'\n",
    "                  ]\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fillWithMode(df):\n",
    "        ''' fill missing values with mode, median\n",
    "        '''\n",
    "        df = df.copy()\n",
    "\n",
    "        columns = ['Electrical', 'KitchenQual', 'Exterior1st',\n",
    "                   'Exterior2nd', 'SaleType'\n",
    "                  ]\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "        # filling with median of each 'Neighborhood'\n",
    "        df['LotFrontage'] = (\n",
    "                         df.groupby('Neighborhood')['LotFrontage']\n",
    "                         .transform(lambda x: x.fillna(x.median()))\n",
    "                        )  \n",
    "\n",
    "        # nan means Typical\n",
    "        df['Functional'] = df['Functional'].fillna('Typ')\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mszoning(df):\n",
    "        ''' recives a DF this imputation takes place on test data only'''\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        idotrr = ( (df['Neighborhood'] == 'IDOTRR') & (df['MSZoning'].isna()) )\n",
    "        mitchel = ( (df['Neighborhood'] == 'Mitchel') & (df['MSZoning'].isna()) )\n",
    "\n",
    "        df.loc[ idotrr , 'MSZoning'] = 'RM'\n",
    "        df.loc[ mitchel, 'MSZoning'] = 'RL'\n",
    "\n",
    "    #     # to test this function out of here\n",
    "    #     temp = mszoning(d_te)\n",
    "    #     # lable index acces at [1916, 2217, 2251, 2905\n",
    "    #     temp.loc[[1916, 2217, 2251, 2905], 'MSZoning']\n",
    "\n",
    "        return df\n",
    "\n",
    "    none_func = FunctionTransformer(fillWithNone, validate=False) \n",
    "    zero_func = FunctionTransformer(fillWithZero, validate=False) \n",
    "    mode_func = FunctionTransformer(fillWithMode, validate=False)\n",
    "    mszo_func = FunctionTransformer(mszoning, validate=False)\n",
    "    \n",
    "    \n",
    "    imputer = Pipeline([\n",
    "                    ('withNone', none_func), \n",
    "                    ('withZero', zero_func), \n",
    "                    ('withMode', mode_func), \n",
    "                    ('mszoni', mszo_func)\n",
    "                   ])\n",
    "    \n",
    "    result = imputer.fit_transform(all_data)\n",
    "    \n",
    "    return result.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fe660-3a2d-4178-ad09-ee1ceed3dbbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reduce Categories(Collapsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b2e37e1-4772-4ec5-94f3-3cd2beaac333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_categories(all_data):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "\n",
    "    con1_others = ['RRAn', 'PosN', 'RRAe', 'PosA', 'RRNn', 'RRNe']\n",
    "    roofS_others = ['Gambrel', 'Flat', 'Mansard', 'Shed']\n",
    "    foun_others = ['Slab', 'Stone', 'Wood']\n",
    "    gara_others = ['None', 'Basment', '2Types', 'CarPort']\n",
    "    saleT_others = ['ConLD', 'CWD', 'ConLI', 'ConLw', 'Oth', 'Con']\n",
    "    saleC_others = ['Family', 'Alloca', 'AdjLand']\n",
    "    exte1_others = ['BrkComm', 'AsphShn', 'Stone', 'CBlock', 'ImStucc']\n",
    "    exte2_others = ['BrkComm', 'AsphShn', 'Stone', 'CBlock', 'ImStucc', 'Other']\n",
    "    lotC_others = ['FR2', 'FR3']\n",
    "\n",
    "    all_data['Condition1'] = all_data['Condition1'].map(lambda x: 'Others' if x in con1_others else x)\n",
    "    all_data['RoofStyle'] = all_data['RoofStyle'].map(lambda x: 'Others' if x in roofS_others else x)\n",
    "    all_data['Foundation'] = all_data['Foundation'].map(lambda x: 'Others' if x in foun_others else x)\n",
    "    all_data['GarageType'] = all_data['GarageType'].map(lambda x: 'Others' if x in gara_others else x)\n",
    "    all_data['SaleType'] = all_data['SaleType'].map(lambda x: 'Others' if x in saleT_others else x)\n",
    "    all_data['SaleCondition'] = all_data['SaleCondition'].map(lambda x: 'Others' if x in saleC_others else x)\n",
    "    all_data['Exterior1st'] = all_data['Exterior1st'].map(lambda x: 'Others' if x in exte1_others else x)\n",
    "    all_data['Exterior2nd'] = all_data['Exterior2nd'].map(lambda x: 'Others' if x in exte2_others else x)\n",
    "    all_data['LotConfig'] = all_data['LotConfig'].map(lambda x: 'Others' if x in lotC_others else x)\n",
    "\n",
    "\n",
    "    # Reducing to a BINARY CLASSES(just 2 clases)\n",
    "\n",
    "    landC_others = ['HLS', 'Bnk', 'Low']\n",
    "    cond2_others = ['Feedr', 'Artery', 'PosN', 'PosA', 'RRNn', 'RRAn', 'RRAe']\n",
    "    roofM_others = ['Tar&Grv', 'WdShake', 'WdShngl', 'Metal', 'Membran', 'Roll', 'ClyTile']\n",
    "    heati_others = ['GasW', 'Grav', 'Wall', 'OthW', 'Floor']\n",
    "    elect_others = ['FuseA', 'FuseF', 'FuseP', 'Mix']\n",
    "    miscF_others = ['Shed', 'Gar2', 'Othr', 'TenC']\n",
    "\n",
    "    all_data['LandContour'] = all_data['LandContour'].map(lambda x: 'Others' if x in landC_others else x)\n",
    "    all_data['Condition2'] = all_data['Condition2'].map(lambda x: 'Others' if x in cond2_others else x)\n",
    "    all_data['RoofMatl'] = all_data['RoofMatl'].map(lambda x: 'Others' if x in roofM_others else x)\n",
    "    all_data['Heating'] = all_data['Heating'].map(lambda x: 'Others' if x in heati_others else x)\n",
    "    all_data['Electrical'] = all_data['Electrical'].map(lambda x: 'Others' if x in elect_others else x)\n",
    "\n",
    "    all_data['MiscFeature'] = all_data['MiscFeature'].map(lambda x: 'Others' if x in miscF_others else x)\n",
    "    \n",
    "    \n",
    "    return all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f29a9b-652d-4150-b0cd-d8208223ca2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bb5c6fd-4af0-459c-8cc6-ab0ee2914b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features_engineering(all_data):\n",
    "    # Before using d_tr set \n",
    "    # get the new d_tr set from all_data\n",
    "    d_tr = all_data[all_data['Training']].copy()\n",
    "\n",
    "    # Creating a new feature 'PeakMonths', 'Unfinished',\n",
    "    # 'Splited', and TotalSF\n",
    "    peak_moS = [5, 6, 7]\n",
    "    unfi_hou = ['1.5Unf', '2.5Unf']\n",
    "    spli_hou = ['SFoyer', 'SLvl']\n",
    "\n",
    "    all_data['PeakMonths'] = all_data['MoSold'].map(lambda x: 'Peak' if x in peak_moS else 'Normal' )\n",
    "    all_data['Finished']   = all_data['HouseStyle'].map(lambda x: 'no' if x in unfi_hou else 'yes') \n",
    "    all_data['Splited']    = all_data['HouseStyle'].map(lambda x: 'yes' if x in spli_hou else 'no')\n",
    "\n",
    "    all_data['TotalSF']    = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "    \n",
    "    # ======== Clustering ==========\n",
    "    # we will use the training data because if we uses all_data\n",
    "    # it has 'nan' in 'SalePrice'. test data does not have 'SalePrice'\n",
    "    # 'Neighborhood', 5, 'MSSubClass', 4\n",
    "    nei_cluster = make_clusters(d_tr.copy(), 'Neighborhood', 'SalePrice', 5)\n",
    "    mss_cluster = make_clusters(d_tr.copy(), 'MSSubClass', 'SalePrice', 4)\n",
    "\n",
    "    # merging the clusters data frame with all_data DataFrame\n",
    "    # we got a 'nan' cluster becouse 'MSSubClass' in test_DF\n",
    "    # has a '150' class that is just in test\n",
    "    # we preserved the index from all_d DF\n",
    "    all_data = all_data.reset_index().merge(nei_cluster, how='left', on='Neighborhood').set_index('Id')\n",
    "    # all_data.drop(columns='Neighborhood', inplace=True)\n",
    "\n",
    "    all_data = all_data.reset_index().merge(mss_cluster, how='left', on='MSSubClass').set_index('Id')\n",
    "    \n",
    "    # dropping old columns\n",
    "    # all_data.drop(columns=['Neighborhood', 'MSSubClass'], inplace=True)\n",
    "\n",
    "    # all_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # ========= Cycling Features ====\n",
    "    all_data[ 'MoSold' + '_sin'] = np.sin( all_data['MoSold'] * (2.*np.pi/12) )\n",
    "    all_data[ 'MoSold' + '_cos'] = np.cos( all_data['MoSold'] * (2.*np.pi/12) )\n",
    "    \n",
    "    # ========= Cleaning ============\n",
    "    all_data.drop(columns=['Neighborhood', 'MSSubClass', 'MoSold'], inplace=True)\n",
    "    \n",
    "    # updating the ** new feature ** types\n",
    "    all_data = all_data.astype( {'PeakMonths':str, 'Finished':str, 'Splited':str,\n",
    "                                 'Neighborhood_Cluster': str, 'MSSubClass_Cluster': str} )#,\n",
    "                                 # 'MoSold_sin': str, 'MoSold_cos': str } )\n",
    "    \n",
    "    return all_data.copy()\n",
    "\n",
    "# # updating cat_to_1Hot\n",
    "# cat_to_1Hot.update( {'PeakMonths':str, 'Finished':str, 'Splited':str,\n",
    "#                        'Neighborhood_Cluster': str, 'MSSubClass_Cluster': str} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2753745-3428-4207-bc5b-661943b375f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Global Variables\n",
    "> ord_cat_mapping  \n",
    "> cat_to_1Hot  \n",
    "> ord_cat_DONE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bd0b41-503b-4d5d-9c6e-2e4c221f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ****** All writed by me ****** #####\n",
    "##########################################\n",
    "def get_global_variables():\n",
    "    ord_cat_mapping = [\n",
    "        {\n",
    "            'col': 'FireplaceQu',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        },\n",
    "        {\n",
    "            'col': 'GarageQual',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        },\n",
    "        {\n",
    "            'col': 'GarageCond',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtFinType1',\n",
    "            'mapping': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtFinType2',\n",
    "            'mapping': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "        },\n",
    "        {\n",
    "            'col': 'ExterQual',\n",
    "            'mapping': {'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'ExterCond',\n",
    "            'mapping': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtQual',\n",
    "            'mapping': {'None': 0 , 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtCond',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'PoolQC',\n",
    "            'mapping': {'None': 0, 'Fa': 1, 'Gd': 2, 'Ex': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'HeatingQC',\n",
    "            'mapping': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'KitchenQual',\n",
    "            'mapping': {'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtExposure',\n",
    "            'mapping': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Functional',\n",
    "            'mapping': {'Sev': 0, 'Maj2': 1, 'Maj1': 2, 'Mod': 3, 'Min2': 4, 'Min1': 5, 'Typ': 6}\n",
    "        },\n",
    "        {\n",
    "            'col': 'GarageFinish',\n",
    "            'mapping': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Fence',\n",
    "            'mapping': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'CentralAir',\n",
    "            'mapping': {'N': 0, 'Y': 1}\n",
    "        },\n",
    "        {\n",
    "            'col': 'PavedDrive',\n",
    "            'mapping': {'N': 0, 'P': 1, 'Y': 2}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Street',\n",
    "            'mapping': {'Grvl': 0, 'Pave': 1}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Alley',\n",
    "            'mapping': {'None': 0, 'Grvl': 1, 'Pave': 2}\n",
    "        },\n",
    "        {\n",
    "            'col': 'LandSlope',\n",
    "            'mapping': {'Gtl': 0, 'Mod': 1, 'Sev': 2}\n",
    "        },\n",
    "        {\n",
    "            'col': 'LotShape',\n",
    "            'mapping': {'Reg': 0, 'IR1': 1, 'IR2': 2, 'IR3': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'HouseStyle', \n",
    "            'mapping': {'SLvl': 0, 'SFoyer': 0, '1Story': 1, '1.5Fin': 2, \n",
    "                        '1.5Unf': 2, '2Story': 3, '2.5Unf': 4, '2.5Fin': 4}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # list of categorical columns(23)  \n",
    "    # that we just encoded\n",
    "    ord_cat_DONE = {'FireplaceQu': str, 'GarageQual': str,'GarageCond': str,'BsmtFinType1': str,\n",
    "                     'BsmtFinType2': str,'ExterQual': str,'ExterCond': str,'BsmtQual': str,\n",
    "                     'BsmtCond': str,'PoolQC': str,'HeatingQC': str,'KitchenQual': str,\n",
    "                     'BsmtExposure': str,'Functional': str,'GarageFinish': str,'Fence': str, \n",
    "                     'CentralAir': str, 'PavedDrive': str,'Street': str,'Alley': str,\n",
    "                     'LandSlope': str,'LotShape': str, 'HouseStyle': str}\n",
    "\n",
    "    # to encode using OneHot (15 so far)\n",
    "    cat_to_1Hot = {'Condition1': str, 'RoofStyle': str, 'Foundation': str, 'GarageType': str, 'SaleType': str, \n",
    "                   'SaleCondition': str, 'Exterior1st': str, 'Exterior2nd': str, 'LotConfig': str, 'LandContour': str, \n",
    "                   'Condition2': str, 'RoofMatl': str, 'Heating': str, 'Electrical': str, 'MiscFeature': str }\n",
    "\n",
    "    # updating cat_to_1Hot with new created features\n",
    "    cat_to_1Hot.update( {'PeakMonths':str, 'Finished':str, 'Splited':str,\n",
    "                           'Neighborhood_Cluster': str, 'MSSubClass_Cluster': str} )\n",
    "\n",
    "    # updating the types\n",
    "    # update the list of ordinal with 2 columns name\n",
    "    # that allredy are ordinal and encoded ('OverallQual', 'OverallCond')\n",
    "\n",
    "    ord_cat_DONE.update({'OverallQual': str, 'OverallCond': str})#,\n",
    "                         # 'MoSold_sin': str, 'MoSold_cos': str})\n",
    "    # new_d = new_d.astype(ord_cat_DONE)\n",
    "\n",
    "    # 5 more columns will concidere as categorical\n",
    "    # we take off 'MoSold': str, \n",
    "    cat_to_1Hot.update({'YrSold': str, 'BldgType':str, \n",
    "                        'MSZoning': str, 'MasVnrType': str})\n",
    "\n",
    "    # new_d = new_d.astype(cat_to_1Hot)\n",
    "    \n",
    "    # updating the data type\n",
    "    # df = df.astype(cat_to_1Hot)\n",
    "    # df = df.astype(ord_cat_DONE)\n",
    "    \n",
    "    return (ord_cat_mapping, ord_cat_DONE, cat_to_1Hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0837ed-4ff2-4ff0-8757-22bd1ab8858e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Ordinal and OneHot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb0ea81b-798a-4140-b820-f079c439f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_1hot_encode(all_data, ord_mapping, oneHot_col):\n",
    "    \n",
    "    ## OrdinalEncoder\n",
    "    oe = OrdinalEncoder(mapping=ord_mapping).fit(all_data)\n",
    "    \n",
    "    all_data = oe.transform(all_data)\n",
    "    \n",
    "    \n",
    "    ## OneHot encoding\n",
    "    oh = OneHotEncoder(cols=oneHot_col).fit(all_data)\n",
    "    \n",
    "    all_data = oh.transform(all_data.copy())\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf713e-99a7-4ca7-9461-f8baf513f1fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0ae2292-19fb-42fe-bc1e-9f5853c623f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, verbose=False, sorted_features=False):\n",
    "    \n",
    "    mutual_info = mutual_info_regression(X=X, y=y)\n",
    "    \n",
    "    mu_info_df = pd.DataFrame(list(zip( X.columns, mutual_info )), columns=['Features', 'Mutual_info'])\n",
    "    \n",
    "    feature_to_dop = mu_info_df[ mu_info_df['Mutual_info'] == 0]\n",
    "    feature_to_dop = list( feature_to_dop['Features'] )\n",
    "    \n",
    "    if(verbose):\n",
    "        #Sorting\n",
    "        # mu_info_df.sort_values('Mutual_info', ascending=False, inplace=True)\n",
    "        print('{} Features with Zero(0) mutual info: \\n'.format(len(feature_to_dop)))\n",
    "        feature_to_dop = sorted(feature_to_dop)\n",
    "        print(feature_to_dop)\n",
    "        \n",
    "    if( (verbose == False) and sorted_features):\n",
    "        feature_to_dop = sorted(feature_to_dop)\n",
    "        \n",
    "    \n",
    "    return (feature_to_dop, mu_info_df.sort_values('Mutual_info', ascending=False).copy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaf039-97dd-424a-9acc-8e29383765df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fix skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f22fb205-9101-4bfc-bc98-fde5ed0cf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_skew_cols(df, cols, treshold=0.7):\n",
    "    '''\n",
    "    this function scals and fix the skewed columns\n",
    "    by applying RobustScaler() and 'yeo-johnson' transform\n",
    "    '''\n",
    "    \n",
    "    skewed = all_d[cols].skew()\n",
    "    \n",
    "    skewed = skewed[skewed >= 0.7]\n",
    "    skewed = skewed.index\n",
    "    \n",
    "    print('{} features with skewe >= {}'.format( len(skewed), treshold ) )\n",
    "    \n",
    "    fix_data_skew = Pipeline([('scaler', RobustScaler()), \n",
    "                          ('yeo', PowerTransformer(method='yeo-johnson'))])\n",
    "    \n",
    "    df[skewed] = fix_data_skew.fit_transform(df[skewed].copy())\n",
    "    \n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3532270-0c04-4774-8e29-9515299c72f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16a2582-b73f-46b2-a2c6-987524757bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features(df, top_cols):\n",
    "    '''\n",
    "    function to create the interaction feature of the top_cols\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    top_cols_set = df[top_cols].copy()\n",
    "    # top_cols_set\n",
    "    \n",
    "    poly_features = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "    \n",
    "    index_backup = top_cols_set.index.copy()\n",
    "    \n",
    "    poly_set = pd.DataFrame( poly_features.fit_transform(top_cols_set.copy()), columns=poly_features.get_feature_names_out(top_cols) )\n",
    "    poly_set.set_index(index_backup, inplace=True)\n",
    "    # poly_set\n",
    "    \n",
    "    poly_set.drop(columns=top_cols, inplace=True)\n",
    "    # poly_set\n",
    "    \n",
    "    result = df.merge(poly_set, left_index=True, right_index=True)\n",
    "    \n",
    "    return result.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b9e1c-5469-49f1-a37d-97d33dc29cd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af68c83-8ecc-46e4-8d5b-1fc378142d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_tr shape: (1460, 80)\n",
      "d_te shape: (1459, 79)\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "d_tr, d_te = data(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d170f1fd-2d3a-4c47-b7b8-850af2319f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droped index: \n",
      " Int64Index([524, 1299], dtype='int64', name='Id')\n"
     ]
    }
   ],
   "source": [
    "# taking cara of some outliers\n",
    "# and making 'y' more gausian like\n",
    "d_tr = analyze_y(d_tr.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b937b56-d08f-46b3-9506-022753886a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data shape:  (2917, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2917, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after take care of some outlier\n",
    "# combine the training and the test\n",
    "all_d = combine_tr_te(d_tr.copy(), d_te.copy(), verbose=True)\n",
    "all_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819f332-8413-474d-8f84-fa05967cdc9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baf1aea8-67b8-46c2-9d9e-652d48a3e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing nan\n",
    "all_d = impute_nan(all_d.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc0c2f0a-1380-44bd-ba48-e68cfbdac267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SalePrice']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_d.columns[all_d.isna().any()].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db751d16-b994-4e52-91af-fbe8eabe5639",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reducing Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86001183-b5b7-452d-9622-49c154193ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the number of categories in some columns\n",
    "all_d = reduce_categories(all_d.copy())\n",
    "# all_d['Foundation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466796c-7055-421b-b38d-e0995b1228ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71e9dd6d-6a96-4dc8-9902-22bc5ef269aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature 'PeakMonths', 'Unfinished',\n",
    "# 'Splited', and TotalSF, and Clusters \n",
    "# transforming cyclical feature 'MoSold'\n",
    "all_d = features_engineering(all_d.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542211c3-9dcf-4e24-8e48-c286c4b409e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Identifying  \n",
    "> Categorical and Number Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1647b158-d279-4add-9b25-332a7a3f5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the global variables\n",
    "ord_cat_mapping, ord_cat_DONE, cat_to_1Hot = get_global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e1c18f3-988c-48f9-94ef-5bd620ae359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the data type\n",
    "all_d = all_d.astype(cat_to_1Hot)\n",
    "all_d = all_d.astype(ord_cat_DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce1c448-e544-4f53-8b6a-bd105ba121e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after updating all the categorical columns\n",
    "# we can now identify the number columns\n",
    "numbers_col = all_d.select_dtypes('number')\n",
    "numbers_col = numbers_col.columns.to_list()\n",
    "\n",
    "# removing the transformed cyclical feature\n",
    "# that already are float and they were identifyed\n",
    "# as numerical features and we dont want to\n",
    "# touch them\n",
    "numbers_col.remove('MoSold_sin')\n",
    "numbers_col.remove('MoSold_cos')\n",
    "# MoSold_sin, MoSold_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710eaf2-b38f-4fc9-8083-c706286e948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_col = numbers_col + list(cat_to_1Hot.keys()) + list(ord_cat_DONE.keys())\n",
    "set( all_d.columns.to_list() ) - set( analyzed_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbe87d-f5a5-4c34-8209-a9f03eee4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set( analyzed_col ) - set( all_d.columns.to_list() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f622b3-79ed-4e28-bd28-752a0efb223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('analyzed_col len:', len(analyzed_col))\n",
    "print('all_d.columns len', len(all_d.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bee38-a49d-4939-97ca-c5004cf1d1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e22f6db8-83ed-41cc-a4e3-5b279e462120",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fixing skewnnes on Number Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "427948b2-3178-49dc-b207-da2386c596f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 features with skewe >= 0.7\n"
     ]
    }
   ],
   "source": [
    "# the DF returned will have numbers_col Scalered and \n",
    "# PowerTransformed\n",
    "all_d = fix_skew_cols(all_d.copy(), numbers_col) # this function keeps the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d4c33-b05d-4356-b86a-0f5744bbc59e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Doing Ordinal and OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44b4b2ba-004b-4127-a973-9e5370fbf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding ordinal and onehot columns( keeps Id index)\n",
    "all_d = ordinal_1hot_encode(all_d.copy(), ord_cat_mapping, cat_to_1Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefa6fc-51fb-4797-8295-074b9957dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e36385-2043-4eb7-9186-a88801ad7762",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31f5d342-f107-449a-b20f-4faabb95a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set to work with\n",
    "d_tr = all_d[ all_d['Training'] ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfde5e36-121f-4d33-9345-b40f3ad4c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 Features with Zero(0) mutual info: \n",
      "\n",
      "['BsmtFinSF2', 'Condition1_3', 'Condition2_1', 'Exterior1st_10', 'Exterior1st_11', 'Exterior1st_5', 'Exterior1st_9', 'Exterior2nd_12', 'Exterior2nd_3', 'Finished_1', 'Finished_2', 'LandContour_1', 'LotConfig_2', 'LotConfig_3', 'LowQualFinSF', 'MSSubClass_Cluster_2', 'MSZoning_5', 'MiscFeature_1', 'MiscFeature_2', 'MoSold_sin', 'Neighborhood_Cluster_1', 'Neighborhood_Cluster_2', 'PoolArea', 'RoofMatl_1', 'RoofMatl_2', 'RoofStyle_3', 'SaleCondition_2', 'SaleCondition_4', 'SaleType_4', 'YrSold_3', 'YrSold_5']\n"
     ]
    }
   ],
   "source": [
    "# we will get the features that has 0 mutual information with 'y'\n",
    "# and a DF with **ALL the features** with their mutual info value\n",
    "# from it we can get the top ones, to do polynomial interaction\n",
    "to_drop, mutual_info_df = feature_selection( d_tr.drop(columns='SalePrice').copy(), \n",
    "                                            d_tr['SalePrice'].copy(), verbose=True, sorted_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964edab-d281-43c7-a969-a069affd9e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3d54aa-45a8-4e4f-ab9e-fca86c4f8ca7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Polynomial and Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0d7fb1b-6b6c-491f-a3aa-52d62aa49867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TotalSF', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'YearBuilt', 'KitchenQual', 'BsmtQual', 'ExterQual', '1stFlrSF', 'GarageFinish', 'GarageYrBlt', 'FullBath', 'YearRemodAdd', 'TotRmsAbvGrd', 'LotFrontage', 'FireplaceQu', '2ndFlrSF', 'LotArea']\n"
     ]
    }
   ],
   "source": [
    "# from mutual_info_df get the top 20 features\n",
    "# that has the hight mutual information\n",
    "# becouse it is ordered we can take the top 20\n",
    "top_20_cols = mutual_info_df.head(20)['Features'].to_list()\n",
    "print( top_20_cols )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0ebcb65-a0c0-4515-976a-a90972ee8157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2917, 158)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9aff3d48-d9a3-402a-b93a-137ef2cac8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get the interaction factor por the top 20 \n",
    "all_d = poly_features(all_d.copy(), top_20_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d5cdc74-55d9-4794-af61-d592d5fd4b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2917, 348)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656327ea-db89-4f60-8a51-380ef5bbdd7e",
   "metadata": {},
   "source": [
    "> Before modeling drop the features with 0 mutual info  \n",
    "> beacous the list has 'Training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f0097-e11f-471f-8633-80122865f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep 'Training' column to identify training set in all_d\n",
    "to_drop.remove('Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "561f974d-aaef-46a5-908b-08852de6c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aditional features that we dont \n",
    "# want to remove\n",
    "to_drop.remove('MoSold_sin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ceacac9-7a76-459c-8aa6-5a34c365450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2917, 348), 30)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_d.shape, len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0256c69c-1eaf-4725-84ac-ba5ed6a66403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Training' is in to_drop list\n",
    "all_d.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0ba43e4-a468-4a94-82d6-e994accedde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2917, 318)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16a010-7dca-46b9-844e-956452ecbe9c",
   "metadata": {},
   "source": [
    "> Getting **the Training, Test set** and 'y' back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7454bb77-9602-4749-8d13-54f7f2a3591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = all_d[ all_d['Training'] ].copy()\n",
    "te = all_d[  ~all_d['Training'] ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19886b11-ce45-4133-9ca6-fd3dee169109",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tr[ 'SalePrice' ].copy()\n",
    "\n",
    "tr.drop(columns='SalePrice', inplace=True)\n",
    "te.drop(columns='SalePrice', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c900c-1e5a-477a-824b-27f7741c4363",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Outliers detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8364be-6371-487e-a081-24dc9f4069b2",
   "metadata": {},
   "source": [
    "> Using **IsolationForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "516837dd-fd65-4906-a942-0719a413933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39a2d9b8-9feb-47a4-a7d9-86859408175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = iso_forest.fit_predict(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fb8f271-fa31-4144-9eab-d3144f05078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_mask = y_hat != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec4adc8b-3b5c-4f07-96df-0330298d1ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 124 outliers\n"
     ]
    }
   ],
   "source": [
    "print( 'There are {} outliers'.format(np.count_nonzero(y_hat == -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3271f981-4e40-45ee-965d-949e3cfa4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  11   20   35   39   48   53   56   58   87   88  113  118  161  178\n",
      "   185  189  197  224  225  227  231  232  235  261  278  304  313  320\n",
      "   324  335  349  350  362  363  375  378  389  417  430  434  438  440\n",
      "   441  473  477  489  490  496  515  526  528  532  542  580  582  590\n",
      "   598  607  613  634  635  638  641  643  648  660  663  690  701  704\n",
      "   744  746  768  773  797  802  824  828  836  842  849  884  887  897\n",
      "   913  931  933  955 1023 1030 1038 1043 1045 1067 1077 1088 1090 1106\n",
      "  1141 1168 1172 1180 1181 1189 1217 1218 1227 1229 1242 1249 1266 1267\n",
      "  1335 1348 1351 1357 1362 1371 1384 1386 1403 1420 1440 1447]]\n"
     ]
    }
   ],
   "source": [
    "# reshape to print the indexes in a block\n",
    "print(np.argwhere(y_hat == -1).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d020a39-006b-45cf-9692-5264108b2179",
   "metadata": {},
   "source": [
    "> Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "056a0403-99f5-4697-abd8-056bc421d402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458, 317)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ceb84bd6-01f8-4e20-9133-e503068acf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr.iloc[inliers_mask, : ]\n",
    "y  = y[inliers_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "028c81ce-049a-4e56-b2a7-8db29b7758bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334, 317)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb766df2-c68d-4736-a8ed-d3ed07bc7a1f",
   "metadata": {},
   "source": [
    "> using **residuals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fee78418-ae78-4776-8a56-eb231baecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cce8590e-f537-4641-a559-2e1deac96857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(tr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8a123f8-9698-4cf4-b32d-d74dd0b590a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr.predict(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecf55cba-72ac-4b8c-afae-7d70ef037469",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - y_hat\n",
    "standard_residuals = abs(  ( residuals - residuals.mean() ) / residuals.std() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46b11658-9871-48bb-8fc8-13c2b412b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers_mask = standard_residuals <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a58c5dcd-f159-4076-ab5e-f4e9f2bfeea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 16 new outliers\n"
     ]
    }
   ],
   "source": [
    "print( 'there are {} new outliers'.format( np.count_nonzero( (standard_residuals >= 3) ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e8e4511-29a1-423b-8057-00d35e51b717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 411,  463,  561,  589,  629,  633,  682,  689,  715,  773,  969,\n",
       "             971, 1023, 1325, 1433, 1454],\n",
       "           dtype='int64', name='Id')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index dropped\n",
    "inliers_mask[ inliers_mask == False].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6af5b8-cc48-46e7-b278-15ef1f4713fb",
   "metadata": {},
   "source": [
    "> **removing** new outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef7a7777-3bb3-4cc4-b14b-0163c97448f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1334, 317)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c24f9f94-cec3-46e1-aff0-5b23466cf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr.loc[ inliers_mask ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62c33949-0d19-4ea9-9952-b049a3cb2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[ inliers_mask ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57193391-7258-4504-9115-3707582b65ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1318, 317), (1318,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6618fc4-d191-432b-a843-e33f623ef0e4",
   "metadata": {},
   "source": [
    "### Saving training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3edca4e-91c8-4e94-a0cc-8d2c85ac4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3391f1e6-9e89-4e41-9072-2b0e103895d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.to_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/tr.csv', index=True)\n",
    "te.to_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/te.csv', index=True)\n",
    "y.to_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/y.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eec4a19-e556-4a5e-955f-ca07b380b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/tr.csv', index_col='Id')\n",
    "te = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/te.csv', index_col='Id')\n",
    "y = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/y.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acae1ae-5dc9-4310-920b-8ecffed40bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy(dtype='float64').reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea1ef8-39f5-4792-b2be-82f7d20a04f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108400de-4b87-4a9b-9aa2-3cd897e87633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TheilSenRegressor, HuberRegressor, RANSACRegressor, Ridge\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ce571d-66d2-4488-8639-4ec16c86000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0485600-be00-47fa-a26c-f5421df84554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b157ce-0406-4f52-8fa3-fd0e4502f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(tr, y, test_size=0.20, random_state=77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0dca44-83cf-4d20-8a9c-54f01fa22cd1",
   "metadata": {},
   "source": [
    "> **TheilSenRegressor()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c96ebc5c-5706-4f51-ae7f-a218dafd7e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:25.098077297210693, tr score: -0.46514573931232694, test score:0.07519757919341458\n"
     ]
    }
   ],
   "source": [
    "# main paramater to change n_subsamples < 843(samples in this data set)\n",
    "# , max_subpopulation\n",
    "theheil = TheilSenRegressor(max_iter=300\n",
    "                            ,max_subpopulation=5000.0\n",
    "                            ,tol=0.1\n",
    "                            ,n_subsamples=843\n",
    "                            ,random_state=77\n",
    "                            ,n_jobs=-1)\n",
    "\n",
    "scores = cross_validate(theheil, X_tr, y_tr, cv=5, n_jobs=-1, scoring='neg_mean_squared_error' )\n",
    "\n",
    "theheil.fit(X_tr, y_tr)\n",
    "\n",
    "te_score = mean_squared_error(y_te, theheil.predict(X_te))\n",
    "\n",
    "print('fit time:{}, tr score: {}, test score:{}'.format(scores['fit_time'].sum()\n",
    "                                                        , scores['test_score'].mean(), te_score))\n",
    "\n",
    "# best score with this configuration\n",
    "# fit time:26.734277963638306, tr score: -0.46514573931232694, test score:0.07519757919341458\n",
    "# max_iter=300\n",
    "# ,max_subpopulation=5000.0\n",
    "# ,tol=0.1\n",
    "# ,n_subsamples=843 <-***** I changed this the last and I got an improvesment and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe53aac-bea4-48bb-9e5e-a48eed75f237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aaa8da-967b-4154-b662-a4cbfb106912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a071b2f8-d16c-48b8-993e-d946078181ba",
   "metadata": {},
   "source": [
    "> **HuberRegressor()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f328a3c6-2760-4761-8a29-8bb1ba1afb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:27.815916538238525, tr score: -0.1179324538906136, test score:0.13009862471610975\n"
     ]
    }
   ],
   "source": [
    "# epsilon must be >= 1\n",
    "huber = HuberRegressor(alpha= 0.01\n",
    "                       ,epsilon= 1.1\n",
    "                       ,max_iter= 7000\n",
    "                       ,tol= 0.01 )\n",
    "\n",
    "scores = cross_validate(huber, X_tr, y_tr, cv=5, n_jobs=-1, scoring='neg_mean_squared_error' )\n",
    "\n",
    "huber.fit(X_tr, y_tr)\n",
    "\n",
    "te_score = mean_squared_error(y_te, huber.predict(X_te))\n",
    "\n",
    "print('fit time:{}, tr score: {}, test score:{}'.format(scores['fit_time'].sum()\n",
    "                                                        , scores['test_score'].mean(), te_score))\n",
    "\n",
    "# Best score using this configuration\n",
    "# fit time:27.178836584091187, tr score: -0.1179324538906136, test score:0.13009862471610975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d01c8-4e39-49fc-a06e-594d6d7fc18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e7c1321-d515-4f67-9bc8-23aec8d8cf48",
   "metadata": {},
   "source": [
    "> **RANSACRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac958da7-6fd0-49a0-812c-7dfa03d29dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:4.435987710952759, tr score: -0.8177974586369536, test score:0.06437515559173791\n"
     ]
    }
   ],
   "source": [
    "# # median absolute deviation(MAD) is the same for\n",
    "# residual_threshold = None\n",
    "# MAD = np.median( np.absolute( y_tr - np.median(y_tr) ) )\n",
    "# MAD = 0.6013\n",
    "# min_samples < 844 (samples in the data set)\n",
    "\n",
    "# Note: when I used residual_threshold = None(defoul) it got slower\n",
    "# than when I used the MAD*2 for cut off outliers and got the same result\n",
    "\n",
    "ransac = RANSACRegressor(max_trials=75 \n",
    "                         ,min_samples=700\n",
    "                         ,residual_threshold=0.7 #1.2 #0.6013\n",
    "                         ,random_state=77)\n",
    "\n",
    "scores = cross_validate(ransac, X_tr, y_tr, cv=5, n_jobs=-1, scoring='neg_mean_squared_error' )\n",
    "\n",
    "ransac.fit(X_tr, y_tr)\n",
    "\n",
    "te_score = mean_squared_error(y_te, ransac.predict(X_te))\n",
    "\n",
    "print('fit time:{}, tr score: {}, test score:{}'.format(scores['fit_time'].sum()\n",
    "                                                        , scores['test_score'].mean(), te_score))\n",
    "\n",
    "# best score\n",
    "# fit time:4.989835023880005, tr score: -0.8177974586369536, test score:0.06437515559173791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b11ed5-cc56-43dd-9e15-d2f58eb978de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad581312-adb3-430c-a83c-f70b96721958",
   "metadata": {},
   "source": [
    "> **Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7c0d4-eb68-426d-9175-bbd4befb7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
    "    Solver to use in the computational routines:\n",
    "\n",
    "    - 'auto' chooses the solver automatically based on the type of data.\n",
    "\n",
    "    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
    "      coefficients. It is the most stable solver, in particular more stable\n",
    "      for singular matrices than 'cholesky' at the cost of being slower.\n",
    "\n",
    "    - 'cholesky' uses the standard scipy.linalg.solve function to\n",
    "      obtain a closed-form solution.\n",
    "\n",
    "    - 'sparse_cg' uses the conjugate gradient solver as found in\n",
    "      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
    "      more appropriate than 'cholesky' for large-scale data\n",
    "      (possibility to set `tol` and `max_iter`).\n",
    "\n",
    "    - 'lsqr' uses the dedicated regularized least-squares routine\n",
    "      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
    "      procedure.\n",
    "\n",
    "    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
    "      its improved, unbiased version named SAGA. Both methods also use an\n",
    "      iterative procedure, and are often faster than other solvers when\n",
    "      both n_samples and n_features are large. Note that 'sag' and\n",
    "      'saga' fast convergence is only guaranteed on features with\n",
    "      approximately the same scale. You can preprocess the data with a\n",
    "      scaler from sklearn.preprocessing.\n",
    "\n",
    "    - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
    "      `scipy.optimize.minimize`. It can be used only when `positive`\n",
    "      is True.\n",
    "\n",
    "    All solvers except 'svd' support both dense and sparse data. However, only\n",
    "    'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n",
    "    `fit_intercept` is True.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2a54ce09-36f1-4f21-97cb-5e482a56547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:0.10879230499267578, tr score: -0.3295412831184402, test score:0.06117395104775661\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha= 4.0\n",
    "               ,max_iter=500#None\n",
    "               ,tol=0.001\n",
    "               ,solver='auto'#,positive=True\n",
    "               ,random_state=77)\n",
    "\n",
    "\n",
    "scores = cross_validate(ridge, X_tr, y_tr, cv=5, n_jobs=-1, scoring='neg_mean_squared_error' )\n",
    "\n",
    "ridge.fit(X_tr, y_tr)\n",
    "\n",
    "te_score = mean_squared_error(y_te, ridge.predict(X_te))\n",
    "\n",
    "print('fit time:{}, tr score: {}, test score:{}'.format(scores['fit_time'].sum()\n",
    "                                                        , scores['test_score'].mean(), te_score))\n",
    "\n",
    "\n",
    "# fit time:0.10782146453857422, tr score: -0.3295412831184402, test score:0.06117395104775661\n",
    "# alpha 4.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9985ee1-6d20-4991-90fc-85895ffeafb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895 ms ± 22.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "test score:0.06305865413794026  \n",
      "best training score:-0.3207073358290147\n",
      "Best params: {'alpha': 1.18}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ridge = Ridge(random_state=77)\n",
    "\n",
    "ridge_param = {'alpha': np.arange(1.18, 1.21, 0.001) }\n",
    "               # ,'max_iter': None\n",
    "               # ,'tol':0.001\n",
    "               # ,'solver': 'auto'}\n",
    "\n",
    "gs_ridge = GridSearchCV(ridge, param_grid=ridge_param, cv=5, refit=True\n",
    "                        , scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "%timeit gs_ridge.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "te_score = mean_squared_error(y_te, gs_ridge.predict(X_te))\n",
    "\n",
    "print('\\ntest score:{}  \\nbest training score:{}'.format( te_score, gs_ridge.best_score_))\n",
    "print('Best params: {}'.format(gs_ridge.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808450b4-efae-4608-ab2c-cd2c435c7ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90c8c7ae-bc04-4b89-990f-4f8d0b1f988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.18 , 1.181, 1.182, 1.183, 1.184, 1.185, 1.186, 1.187, 1.188,\n",
       "       1.189, 1.19 , 1.191, 1.192, 1.193, 1.194, 1.195, 1.196, 1.197,\n",
       "       1.198, 1.199, 1.2  , 1.201, 1.202, 1.203, 1.204, 1.205, 1.206,\n",
       "       1.207, 1.208, 1.209, 1.21 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1.18, 1.21, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8faf4d4-e84a-4fad-b568-70e1de577d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e7b4b46-665f-4080-bc9a-970ad52f4dc2",
   "metadata": {},
   "source": [
    "> **ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c964e5-b353-4782-bfbb-0d2139d9541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total of inter=900\n",
    "grid = {'l1_ratio': np.arange(0, 1, 0.01)\n",
    "        ,'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.0, 1.0, 10.0, 100.0]}\n",
    "\n",
    "e_net = ElasticNet()\n",
    "\n",
    "rs = RandomizedSearchCV(e_net, param_distributions=grid, cv=5, n_iter=100, refit=True\n",
    "                        , scoring='neg_mean_squared_error', n_jobs=-1, random_state=77)\n",
    "\n",
    "%timeit rs.fit(X_tr, y_tr)\n",
    "\n",
    "# best params\n",
    "# 'l1_ratio': 0.99, 'alpha': 0.001\n",
    "#score -0.06356975787949057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a7dbc9-13b9-4414-b77f-2edc06bb9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:0.05545986134055167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.372e+01, tolerance: 8.170e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "e_net = ElasticNet(alpha=0.001 , l1_ratio=0.99, max_iter=10000)\n",
    "\n",
    "e_net.fit(X_tr, y_tr)\n",
    "\n",
    "te_score = mean_squared_error(y_te, e_net.predict(X_te))\n",
    "\n",
    "print('test score:{}'.format(te_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e414cb2-59df-457c-84ab-95bd7603b229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afd82d-ae7d-4986-b51d-cd9cc566deb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8eebfb8-7782-474d-83e0-6266287c9f48",
   "metadata": {},
   "source": [
    "> **XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6105924-f9e4-4965-8113-3436d9b99447",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalset = [(X_tr, y_tr), (X_te, y_te)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe0c02df-a70c-445f-8117-b725f418f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:0.08080179531969359\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 300 #[100,200,500]\n",
    "          ,'learning_rate': 0.07 # defaul=0.3 range: [0,1] typical: 0.01-0.02\n",
    "          ,'max_depth': 6 #defaul=6 range: [0,∞] typical 3-10\n",
    "          ,'min_child_weight': 1 #defaul=1 range: [0,∞] higher good, too hight underfit\n",
    "          # ,'sampling_method': 'uniform' #defaul=uniform [uniform, gradient_based]\n",
    "          ,'subsample': 1 #defaul=1 range: (0,1] typical 0.5 - 1\n",
    "          ,'colsample_bytree': 1 #defaul=1 range: (0,1]\n",
    "          # ,'colsample_bylevel': 1 #defaul=1 range: (0,1]\n",
    "          # ,'colsample_bynode': 1 #defaul=1 range: (0,1]\n",
    "          ,'eval_metric': mean_squared_error #set by the xgboost acord to 'objective'\n",
    "          # ,'objective': 'reg:squarederror' #default = 'reg:squarederror'\n",
    "          # ,'gamma': 0 #defaul=0 range: [0,∞] the larger the more conservative the algorithm will be\n",
    "          ,'n_jobs': -1\n",
    "          ,'random_state': 77\n",
    "          }\n",
    "\n",
    "xgb_model = XGBRegressor(**params)\n",
    "\n",
    "xgb_model.fit(X_tr, y_tr \n",
    "        ,eval_set=evalset\n",
    "        # ,eval_metric='rmse'\n",
    "        ,verbose=False )\n",
    "\n",
    "te_score = mean_squared_error(y_te, xgb_model.predict(X_te))\n",
    "\n",
    "print('test score:{}'.format(te_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b08087-4cf9-4b55-be97-ef734014377b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8a486-4541-4a67-929d-18b72fdbcfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fed4b-9743-450a-bacb-248f36173cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(random_state = 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09af28-d1ba-4331-9823-491a6552632f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f3aa7-6fad-4c83-b3dc-66cdfadb2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators : int\n",
    "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
    "        rounds.\n",
    "\n",
    "    max_depth :  Optional[int]\n",
    "        Maximum tree depth for base learners.\n",
    "    max_leaves :\n",
    "        Maximum number of leaves; 0 indicates no limit.\n",
    "    max_bin :\n",
    "        If using histogram-based algorithm, maximum number of bins per feature\n",
    "    grow_policy :\n",
    "        Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
    "        depth-wise. 1: favor splitting at nodes with highest loss change.\n",
    "    learning_rate : Optional[float]\n",
    "        Boosting learning rate (xgb's \"eta\")\n",
    "    verbosity : Optional[int]\n",
    "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
    "        Specify the learning task and the corresponding learning objective or\n",
    "        a custom objective function to be used (see note below).\n",
    "    booster: Optional[str]\n",
    "        Specify which booster to use: gbtree, gblinear or dart.\n",
    "    tree_method: Optional[str]\n",
    "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
    "        default, XGBoost will choose the most conservative option available.  It's\n",
    "        recommended to study this option from the parameters document :doc:`tree method\n",
    "        </treemethod>`\n",
    "    n_jobs : Optional[int]\n",
    "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
    "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
    "        balance the threads.  Creating thread contention will significantly slow down both\n",
    "        algorithms.\n",
    "    gamma : Optional[float]\n",
    "        (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
    "        leaf node of the tree.\n",
    "    min_child_weight : Optional[float]\n",
    "        Minimum sum of instance weight(hessian) needed in a child.\n",
    "    max_delta_step : Optional[float]\n",
    "        Maximum delta step we allow each tree's weight estimation to be.\n",
    "    subsample : Optional[float]\n",
    "        Subsample ratio of the training instance.\n",
    "    sampling_method :\n",
    "        Sampling method. Used only by `gpu_hist` tree method.\n",
    "          - `uniform`: select random training instances uniformly.\n",
    "          - `gradient_based` select random training instances with higher probability when\n",
    "            the gradient and hessian are larger. (cf. CatBoost)\n",
    "    colsample_bytree : Optional[float]\n",
    "        Subsample ratio of columns when constructing each tree.\n",
    "    colsample_bylevel : Optional[float]\n",
    "        Subsample ratio of columns for each level.\n",
    "    colsample_bynode : Optional[float]\n",
    "        Subsample ratio of columns for each split.\n",
    "    reg_alpha : Optional[float]\n",
    "        L1 regularization term on weights (xgb's alpha).\n",
    "    reg_lambda : Optional[float]\n",
    "        L2 regularization term on weights (xgb's lambda).\n",
    "    scale_pos_weight : Optional[float]\n",
    "        Balancing of positive and negative weights.\n",
    "    base_score : Optional[float]\n",
    "        The initial prediction score of all instances, global bias.\n",
    "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
    "        Random number seed.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "           Using gblinear booster with shotgun updater is nondeterministic as\n",
    "           it uses Hogwild algorithm.\n",
    "\n",
    "    missing : float, default np.nan\n",
    "        Value in the data which needs to be present as a missing value.\n",
    "    num_parallel_tree: Optional[int]\n",
    "        Used for boosting random forest.\n",
    "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
    "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
    "        for more information.\n",
    "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
    "        Constraints for interaction representing permitted interactions.  The\n",
    "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
    "        3, 4]]``, where each inner list is a group of indices of features that are\n",
    "        allowed to interact with each other.  See :doc:`tutorial\n",
    "        </tutorials/feature_interaction_constraint>` for more information\n",
    "    importance_type: Optional[str]\n",
    "        The feature importance type for the feature_importances\\_ property:\n",
    "\n",
    "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
    "          \"total_cover\".\n",
    "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
    "          without bias.\n",
    "\n",
    "    gpu_id : Optional[int]\n",
    "        Device ordinal.\n",
    "    validate_parameters : Optional[bool]\n",
    "        Give warnings for unknown parameter.\n",
    "    predictor : Optional[str]\n",
    "        Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
    "        gpu_predictor].\n",
    "    enable_categorical : bool\n",
    "\n",
    "        .. versionadded:: 1.5.0\n",
    "\n",
    "        .. note:: This parameter is experimental\n",
    "\n",
    "        Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
    "        should be used to specify categorical data type.  Also, JSON/UBJSON\n",
    "        serialization format is required.\n",
    "\n",
    "    max_cat_to_onehot : Optional[int]\n",
    "\n",
    "        .. versionadded:: 1.6.0\n",
    "\n",
    "        .. note:: This parameter is experimental\n",
    "\n",
    "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
    "        for categorical data.  When number of categories is lesser than the threshold\n",
    "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
    "        into children nodes.  Only relevant for regression and binary classification.\n",
    "        See :doc:`Categorical Data </tutorials/categorical>` for details.\n",
    "\n",
    "    eval_metric : Optional[Union[str, List[str], Callable]]\n",
    "\n",
    "        .. versionadded:: 1.6.0\n",
    "\n",
    "        Metric used for monitoring the training result and early stopping.  It can be a\n",
    "        string or list of strings as names of predefined metric in XGBoost (See\n",
    "        doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
    "        user defined metric that looks like `sklearn.metrics`.\n",
    "\n",
    "        If custom objective is also provided, then custom metric should implement the\n",
    "        corresponding reverse link function.\n",
    "\n",
    "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
    "        object is provided, it's assumed to be a cost function and by default XGBoost will\n",
    "        minimize the result during early stopping.\n",
    "\n",
    "        For advanced usage on Early stopping like directly choosing to maximize instead of\n",
    "        minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
    "\n",
    "        See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
    "        for more.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "             This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old one\n",
    "             receives un-transformed prediction regardless of whether custom objective is\n",
    "             being used.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from sklearn.datasets import load_diabetes\n",
    "            from sklearn.metrics import mean_absolute_error\n",
    "            X, y = load_diabetes(return_X_y=True)\n",
    "            reg = xgb.XGBRegressor(\n",
    "                tree_method=\"hist\",\n",
    "                eval_metric=mean_absolute_error,\n",
    "            )\n",
    "            reg.fit(X, y, eval_set=[(X, y)])\n",
    "\n",
    "    early_stopping_rounds : Optional[int]\n",
    "\n",
    "        .. versionadded:: 1.6.0\n",
    "\n",
    "        Activates early stopping. Validation metric needs to improve at least once in\n",
    "        every **early_stopping_rounds** round(s) to continue training.  Requires at least\n",
    "        one item in **eval_set** in :py:meth:`fit`.\n",
    "\n",
    "        The method returns the model from the last iteration (not the best one).  If\n",
    "        there's more than one item in **eval_set**, the last entry will be used for early\n",
    "        stopping.  If there's more than one metric in **eval_metric**, the last metric\n",
    "        will be used for early stopping.\n",
    "\n",
    "        If early stopping occurs, the model will have three additional fields:\n",
    "        :py:attr:`best_score`, :py:attr:`best_iteration` and\n",
    "        :py:attr:`best_ntree_limit`.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
    "\n",
    "    callbacks : Optional[List[TrainingCallback]]\n",
    "        List of callback functions that are applied at end of each iteration.\n",
    "        It is possible to use predefined callbacks by using\n",
    "        :ref:`Callback API <callback_api>`.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "           States in callback are not preserved during training, which means callback\n",
    "           objects can not be reused for multiple training sessions without\n",
    "           reinitialization or deepcopy.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            for params in parameters_grid:\n",
    "                # be sure to (re)initialize the callbacks before each run\n",
    "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
    "                xgboost.train(params, Xy, callbacks=callbacks)\n",
    "\n",
    "    kwargs : dict, optional\n",
    "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
    "        can be found :doc:`here </parameter>`.\n",
    "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
    "        dict simultaneously will result in a TypeError.\n",
    "\n",
    "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
    "\n",
    "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
    "            that parameters passed via this argument will interact properly\n",
    "            with scikit-learn.\n",
    "\n",
    "        .. note::  Custom objective function\n",
    "\n",
    "            A custom objective function can be provided for the ``objective``\n",
    "            parameter. In this case, it should have the signature\n",
    "            ``objective(y_true, y_pred) -> grad, hess``:\n",
    "\n",
    "            y_true: array_like of shape [n_samples]\n",
    "                The target values\n",
    "            y_pred: array_like of shape [n_samples]\n",
    "                The predicted values\n",
    "\n",
    "            grad: array_like of shape [n_samples]\n",
    "                The value of the gradient for each sample point.\n",
    "            hess: array_like of shape [n_samples]\n",
    "                The value of the second derivative for each sample point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895253db-2ece-4a02-a724-aa405ce311e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "22e5a500-216e-4417-bad2-ec7aadab3501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "965    True\n",
       "966    True\n",
       "967    True\n",
       "968    True\n",
       "970    True\n",
       "972    True\n",
       "973    True\n",
       "974    True\n",
       "975    True\n",
       "Name: Training, dtype: bool"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tr.loc[965:975, 'Training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2730c0-635a-4c73-92c1-863b3abf73a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c8e4b-6922-4e49-a335-e9e3d3d67cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6ab1e3c-2073-4df1-bf3f-bb4c8fc6fbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "965    0.685044\n",
       "966    0.237180\n",
       "967   -0.056528\n",
       "968   -0.533016\n",
       "970   -0.429583\n",
       "972    0.150980\n",
       "973   -1.381308\n",
       "974    0.280746\n",
       "975    0.066367\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.loc[965:975]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed0cb687-a849-43aa-90ad-69ecb1594169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "1453   -0.330188\n",
       "1455    0.321856\n",
       "1456    0.180704\n",
       "1457    0.630526\n",
       "1458    1.175759\n",
       "1459   -0.386845\n",
       "1460   -0.282014\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7295b-645a-4ac2-a13e-30c8919dcf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
