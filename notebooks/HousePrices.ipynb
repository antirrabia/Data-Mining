{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9a821-251f-4f1d-b5e6-5f173b9e9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from category_encoders import OrdinalEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca15dd1-30af-4513-ae71-96c8ee6b2e4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcdacc-0e24-43e4-81e6-712cf90428e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clusters(df, col, y, n_cluster, merge=False):\n",
    "    '''\n",
    "        function to boil down a colum to n_cluster\n",
    "        \n",
    "        recive:\n",
    "        \n",
    "            df: a data frame \n",
    "            col: col to make cluster\n",
    "            y: the response variable\n",
    "            n_cluster: stamitation of number of cluster\n",
    "            \n",
    "        return:  \n",
    "            a data frame with 'col' droped, and 'col' + '_cluster' column\n",
    "            is added to the data frame\n",
    "\n",
    "        \n",
    "    '''\n",
    "    col_clusters = KMeans(n_clusters=n_cluster, random_state=777)\n",
    "\n",
    "    # 'Neighborhood' and 'MSSubClass' stats\n",
    "    col_stats = df.groupby(col)[y].describe()\n",
    "\n",
    "    # Getting clusters\n",
    "    col_clusters.fit(col_stats)\n",
    "\n",
    "    # preparing DF with cluster lables to merge\n",
    "    new_name = col + '_Cluster'\n",
    "    col_cluster_df = pd.DataFrame( { col: col_stats.index.to_list(),\n",
    "                                     new_name: col_clusters.labels_.tolist()} )\n",
    "    \n",
    "    if merge:\n",
    "        # merging the clusters with the data frame\n",
    "        df = df.merge(col_cluster_df, how='left', on=col)\n",
    "\n",
    "        df[new_name] = df[new_name].astype(str)\n",
    "        \n",
    "        result = df.drop(columns=col)\n",
    "        \n",
    "    else:\n",
    "        result = col_cluster_df\n",
    "    \n",
    "    return result.copy()\n",
    "\n",
    "\n",
    "# Low frequency categories to 'Other'\n",
    "def lower_than(df, col, percentage):\n",
    "    ''' function that will merge low frequency classes into \n",
    "        a single class 'Others'\n",
    "\n",
    "        parameters:\n",
    "            df: a DataFrame\n",
    "            col: column's name to work on\n",
    "            percentage(%): the threshold like 0.1, 0.2\n",
    "\n",
    "        returns:\n",
    "            df: the data frame with col's classes that are lowers\n",
    "                than 'threshold' been repleced with 'Other' category\n",
    "    '''\n",
    "\n",
    "    # calculating the column frequency\n",
    "    col_freq = df[col].value_counts(normalize=True)\n",
    "\n",
    "    # the getting the column threshold \n",
    "    threshold = col_freq.quantile(q= percentage)\n",
    "\n",
    "    # knowing the classes that are below the threshold\n",
    "    less_freq_classes = col_freq[ col_freq <= threshold ]\n",
    "\n",
    "    others = less_freq_classes.index.to_list()\n",
    "\n",
    "    print(others)\n",
    "\n",
    "    df[col] = df[col].replace(others, 'Others')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef4fc7-4204-4d8c-831b-e2e51c0ff471",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52feccc9-d49a-424c-91d7-7a1256354911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(verbose=False):\n",
    "    \n",
    "    d_tr = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/train.csv', index_col='Id')\n",
    "    d_te = pd.read_csv('/Users/antirrabia/Documents/01-GitHub/DataMining-_-/CSV/HousePrices/test.csv', index_col='Id')\n",
    "    \n",
    "    # Utilities has just 2 categories, and one of them\n",
    "    # just appears once so we delete the whole column.\n",
    "    d_tr = d_tr.drop(columns='Utilities')\n",
    "    d_te = d_te.drop(columns='Utilities')\n",
    "    \n",
    "    # Marking Training Set\n",
    "    d_tr['Training'] = True\n",
    "    d_te['Training'] = False  \n",
    "    \n",
    "    if(verbose):\n",
    "        print('d_tr shape:', d_tr.shape)\n",
    "        print('d_te shape:', d_te.shape)\n",
    "        \n",
    "    return (d_tr.copy(), d_te.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94dca3a-3bcd-4417-ba6a-079ff16ee895",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Analazy 'Y'('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17f4ce-7db5-4633-8cf2-77153160f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_y(df, verbose=False):\n",
    "    \n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Plotting\n",
    "    if(verbose):\n",
    "        fg, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(11,7))\n",
    "        # chequing the distribution of 'y' = 'SalePrice'\n",
    "        \n",
    "        sns.histplot( df['SalePrice'], bins=50, ax= ax1);\n",
    "        ax1.title.set_text('Skewed SalePrice')\n",
    "    \n",
    "        sns.scatterplot(x=df['GrLivArea'], y=df['SalePrice'], ax=ax3);\n",
    "        ax3.title.set_text('Outliers')\n",
    "    \n",
    "    outliers = df[ df['GrLivArea'] >= 4500].index\n",
    "    \n",
    "    # deleting the outliers in 'GrLivArea'\n",
    "    df.drop(outliers, inplace=True)\n",
    "    \n",
    "    # Plotting\n",
    "    if(verbose):\n",
    "        # cheking again\n",
    "        sns.scatterplot(x=df['GrLivArea'], y=df['SalePrice'], ax=ax4);\n",
    "        ax4.title.set_text('NO Outliers')\n",
    "    \n",
    "    # pipeline to scale and do a powerTransforme on y('SalePrice')\n",
    "    fix_y = Pipeline([('scaler', RobustScaler()), ('power', PowerTransformer(method='yeo-johnson'))])\n",
    "    \n",
    "    y = fix_y.fit_transform( df['SalePrice'].values.reshape(-1, 1) )\n",
    "    \n",
    "    # Plotting\n",
    "    if(verbose):\n",
    "        # checking the distribution of 'y' again\n",
    "        sns.histplot(y, bins=50, ax=ax2);\n",
    "        ax2.title.set_text('yeo-johnson transformed \\'y\\'')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # if(verbose):\n",
    "    print('droped index: \\n', outliers)\n",
    "        \n",
    "    df['SalePrice'] = y.copy()\n",
    "    \n",
    "    return df.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4e36f-2405-4d18-9056-5c998eb3e7ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Combining training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e0f7d-a6db-49df-ae96-26052ea2a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tr_te(df_tr, df_te, verbose=False):\n",
    "    \n",
    "    all_d = pd.concat([d_tr.copy(), d_te.copy()])\n",
    "    \n",
    "    if(verbose):\n",
    "        print('New data shape: ', all_d.shape)\n",
    "        \n",
    "    return all_d.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ade99-7886-4637-89f1-9c7b0dff611d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imputting 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f383d9-7b69-4ad6-a79a-605c0c65c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_nan(all_data):\n",
    "    \n",
    "    # 34 columns with nan\n",
    "    def fillWithNone(df):\n",
    "        ''' nan in 'PoolQC' means 'no pool' \n",
    "            nan in 'MiscFeature' means 'no misc feature'\n",
    "            nan in 'Alley' means 'no alley acces'\n",
    "            nan in 'Fence' means 'no fence'\n",
    "            nan in 'FireplaceQu' means 'no Fireplace'\n",
    "            nan in 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1'\n",
    "                'BsmtFinType1', 'MasVnrType', 'MSSubClass'\n",
    "                'GarageCond' replaced with 'None' too\n",
    "\n",
    "            recive a df\n",
    "        '''\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        columns = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n",
    "                   'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "                   'GarageQual', 'GarageCond', 'BsmtQual',\n",
    "                   'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "                   'BsmtFinType2', 'MasVnrType'\n",
    "                  ]\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = df[col].fillna('None')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fillWithZero(df):\n",
    "        ''' nan \n",
    "\n",
    "        '''\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        columns = ['GarageYrBlt', 'GarageArea', 'GarageCars',\n",
    "                   'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                   'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n",
    "                   'MasVnrArea'\n",
    "                  ]\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fillWithMode(df):\n",
    "        ''' fill missing values with mode, median\n",
    "        '''\n",
    "        df = df.copy()\n",
    "\n",
    "        columns = ['Electrical', 'KitchenQual', 'Exterior1st',\n",
    "                   'Exterior2nd', 'SaleType'\n",
    "                  ]\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "        # filling with median of each 'Neighborhood'\n",
    "        df['LotFrontage'] = (\n",
    "                         df.groupby('Neighborhood')['LotFrontage']\n",
    "                         .transform(lambda x: x.fillna(x.median()))\n",
    "                        )  \n",
    "\n",
    "        # nan means Typical\n",
    "        df['Functional'] = df['Functional'].fillna('Typ')\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def mszoning(df):\n",
    "        ''' recives a DF this imputation takes place on test data only'''\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        idotrr = ( (df['Neighborhood'] == 'IDOTRR') & (df['MSZoning'].isna()) )\n",
    "        mitchel = ( (df['Neighborhood'] == 'Mitchel') & (df['MSZoning'].isna()) )\n",
    "\n",
    "        df.loc[ idotrr , 'MSZoning'] = 'RM'\n",
    "        df.loc[ mitchel, 'MSZoning'] = 'RL'\n",
    "\n",
    "    #     # to test this function out of here\n",
    "    #     temp = mszoning(d_te)\n",
    "    #     # lable index acces at [1916, 2217, 2251, 2905\n",
    "    #     temp.loc[[1916, 2217, 2251, 2905], 'MSZoning']\n",
    "\n",
    "        return df\n",
    "\n",
    "    none_func = FunctionTransformer(fillWithNone, validate=False) \n",
    "    zero_func = FunctionTransformer(fillWithZero, validate=False) \n",
    "    mode_func = FunctionTransformer(fillWithMode, validate=False)\n",
    "    mszo_func = FunctionTransformer(mszoning, validate=False)\n",
    "    \n",
    "    \n",
    "    imputer = Pipeline([\n",
    "                    ('withNone', none_func), \n",
    "                    ('withZero', zero_func), \n",
    "                    ('withMode', mode_func), \n",
    "                    ('mszoni', mszo_func)\n",
    "                   ])\n",
    "    \n",
    "    result = imputer.fit_transform(all_data)\n",
    "    \n",
    "    return result.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fe660-3a2d-4178-ad09-ee1ceed3dbbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reduce Categories(Collapsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e37e1-4772-4ec5-94f3-3cd2beaac333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_categories(all_data):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "\n",
    "    con1_others = ['RRAn', 'PosN', 'RRAe', 'PosA', 'RRNn', 'RRNe']\n",
    "    roofS_others = ['Gambrel', 'Flat', 'Mansard', 'Shed']\n",
    "    foun_others = ['Slab', 'Stone', 'Wood']\n",
    "    gara_others = ['None', 'Basment', '2Types', 'CarPort']\n",
    "    saleT_others = ['ConLD', 'CWD', 'ConLI', 'ConLw', 'Oth', 'Con']\n",
    "    saleC_others = ['Family', 'Alloca', 'AdjLand']\n",
    "    exte1_others = ['BrkComm', 'AsphShn', 'Stone', 'CBlock', 'ImStucc']\n",
    "    exte2_others = ['BrkComm', 'AsphShn', 'Stone', 'CBlock', 'ImStucc', 'Other']\n",
    "    lotC_others = ['FR2', 'FR3']\n",
    "\n",
    "    all_data['Condition1'] = all_data['Condition1'].map(lambda x: 'Others' if x in con1_others else x)\n",
    "    all_data['RoofStyle'] = all_data['RoofStyle'].map(lambda x: 'Others' if x in roofS_others else x)\n",
    "    all_data['Foundation'] = all_data['Foundation'].map(lambda x: 'Others' if x in foun_others else x)\n",
    "    all_data['GarageType'] = all_data['GarageType'].map(lambda x: 'Others' if x in gara_others else x)\n",
    "    all_data['SaleType'] = all_data['SaleType'].map(lambda x: 'Others' if x in saleT_others else x)\n",
    "    all_data['SaleCondition'] = all_data['SaleCondition'].map(lambda x: 'Others' if x in saleC_others else x)\n",
    "    all_data['Exterior1st'] = all_data['Exterior1st'].map(lambda x: 'Others' if x in exte1_others else x)\n",
    "    all_data['Exterior2nd'] = all_data['Exterior2nd'].map(lambda x: 'Others' if x in exte2_others else x)\n",
    "    all_data['LotConfig'] = all_data['LotConfig'].map(lambda x: 'Others' if x in lotC_others else x)\n",
    "\n",
    "\n",
    "    # Reducing to a BINARY CLASSES(just 2 clases)\n",
    "\n",
    "    landC_others = ['HLS', 'Bnk', 'Low']\n",
    "    cond2_others = ['Feedr', 'Artery', 'PosN', 'PosA', 'RRNn', 'RRAn', 'RRAe']\n",
    "    roofM_others = ['Tar&Grv', 'WdShake', 'WdShngl', 'Metal', 'Membran', 'Roll', 'ClyTile']\n",
    "    heati_others = ['GasW', 'Grav', 'Wall', 'OthW', 'Floor']\n",
    "    elect_others = ['FuseA', 'FuseF', 'FuseP', 'Mix']\n",
    "    miscF_others = ['Shed', 'Gar2', 'Othr', 'TenC']\n",
    "\n",
    "    all_data['LandContour'] = all_data['LandContour'].map(lambda x: 'Others' if x in landC_others else x)\n",
    "    all_data['Condition2'] = all_data['Condition2'].map(lambda x: 'Others' if x in cond2_others else x)\n",
    "    all_data['RoofMatl'] = all_data['RoofMatl'].map(lambda x: 'Others' if x in roofM_others else x)\n",
    "    all_data['Heating'] = all_data['Heating'].map(lambda x: 'Others' if x in heati_others else x)\n",
    "    all_data['Electrical'] = all_data['Electrical'].map(lambda x: 'Others' if x in elect_others else x)\n",
    "\n",
    "    all_data['MiscFeature'] = all_data['MiscFeature'].map(lambda x: 'Others' if x in miscF_others else x)\n",
    "    \n",
    "    \n",
    "    return all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f29a9b-652d-4150-b0cd-d8208223ca2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5c6fd-4af0-459c-8cc6-ab0ee2914b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features_engineering(all_data):\n",
    "    # Before using d_tr set \n",
    "    # get the new d_tr set from all_data\n",
    "    d_tr = all_data[all_data['Training']].copy()\n",
    "\n",
    "    # Creating a new feature 'PeakMonths', 'Unfinished',\n",
    "    # 'Splited', and TotalSF\n",
    "    peak_moS = [5, 6, 7]\n",
    "    unfi_hou = ['1.5Unf', '2.5Unf']\n",
    "    spli_hou = ['SFoyer', 'SLvl']\n",
    "\n",
    "    all_data['PeakMonths'] = all_data['MoSold'].map(lambda x: 'Peak' if x in peak_moS else 'Normal' )\n",
    "    all_data['Finished']   = all_data['HouseStyle'].map(lambda x: 'no' if x in unfi_hou else 'yes') \n",
    "    all_data['Splited']    = all_data['HouseStyle'].map(lambda x: 'yes' if x in spli_hou else 'no')\n",
    "\n",
    "    all_data['TotalSF']    = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "    \n",
    "    # ======== Clustering ==========\n",
    "    # we will use the training data because if we uses all_data\n",
    "    # it has 'nan' in 'SalePrice'. test data does not have 'SalePrice'\n",
    "    # 'Neighborhood', 5, 'MSSubClass', 4\n",
    "    nei_cluster = make_clusters(d_tr.copy(), 'Neighborhood', 'SalePrice', 5)\n",
    "    mss_cluster = make_clusters(d_tr.copy(), 'MSSubClass', 'SalePrice', 4)\n",
    "\n",
    "    # merging the clusters data frame with all_data DataFrame\n",
    "    # we got a 'nan' cluster becouse 'MSSubClass' in test_DF\n",
    "    # has a '150' class that is just in test\n",
    "    # we preserved the index from all_d DF\n",
    "    all_data = all_data.reset_index().merge(nei_cluster, how='left', on='Neighborhood').set_index('Id')\n",
    "    # all_data.drop(columns='Neighborhood', inplace=True)\n",
    "\n",
    "    all_data = all_data.reset_index().merge(mss_cluster, how='left', on='MSSubClass').set_index('Id')\n",
    "    \n",
    "    # dropping old columns\n",
    "    # all_data.drop(columns=['Neighborhood', 'MSSubClass'], inplace=True)\n",
    "\n",
    "    # all_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # ========= Cycling Features ====\n",
    "    all_data[ 'MoSold' + '_sin'] = np.sin( all_data['MoSold'] * (2.*np.pi/12) )\n",
    "    all_data[ 'MoSold' + '_cos'] = np.cos( all_data['MoSold'] * (2.*np.pi/12) )\n",
    "    \n",
    "    # ========= Cleaning ============\n",
    "    all_data.drop(columns=['Neighborhood', 'MSSubClass', 'MoSold'], inplace=True)\n",
    "    \n",
    "    # updating the ** new feature ** types\n",
    "    all_data = all_data.astype( {'PeakMonths':str, 'Finished':str, 'Splited':str,\n",
    "                                 'Neighborhood_Cluster': str, 'MSSubClass_Cluster': str} )#,\n",
    "                                 # 'MoSold_sin': str, 'MoSold_cos': str } )\n",
    "    \n",
    "    return all_data.copy()\n",
    "\n",
    "# # updating cat_to_1Hot\n",
    "# cat_to_1Hot.update( {'PeakMonths':str, 'Finished':str, 'Splited':str,\n",
    "#                        'Neighborhood_Cluster': str, 'MSSubClass_Cluster': str} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2753745-3428-4207-bc5b-661943b375f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Global Variables\n",
    "> ord_cat_mapping  \n",
    "> cat_to_1Hot  \n",
    "> ord_cat_DONE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd0b41-503b-4d5d-9c6e-2e4c221f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ****** All writed by me ****** #####\n",
    "##########################################\n",
    "def get_global_variables():\n",
    "    ord_cat_mapping = [\n",
    "        {\n",
    "            'col': 'FireplaceQu',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        },\n",
    "        {\n",
    "            'col': 'GarageQual',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        },\n",
    "        {\n",
    "            'col': 'GarageCond',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtFinType1',\n",
    "            'mapping': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtFinType2',\n",
    "            'mapping': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "        },\n",
    "        {\n",
    "            'col': 'ExterQual',\n",
    "            'mapping': {'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'ExterCond',\n",
    "            'mapping': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtQual',\n",
    "            'mapping': {'None': 0 , 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtCond',\n",
    "            'mapping': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'PoolQC',\n",
    "            'mapping': {'None': 0, 'Fa': 1, 'Gd': 2, 'Ex': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'HeatingQC',\n",
    "            'mapping': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'KitchenQual',\n",
    "            'mapping': {'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'BsmtExposure',\n",
    "            'mapping': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Functional',\n",
    "            'mapping': {'Sev': 0, 'Maj2': 1, 'Maj1': 2, 'Mod': 3, 'Min2': 4, 'Min1': 5, 'Typ': 6}\n",
    "        },\n",
    "        {\n",
    "            'col': 'GarageFinish',\n",
    "            'mapping': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Fence',\n",
    "            'mapping': {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n",
    "        },\n",
    "        {\n",
    "            'col': 'CentralAir',\n",
    "            'mapping': {'N': 0, 'Y': 1}\n",
    "        },\n",
    "        {\n",
    "            'col': 'PavedDrive',\n",
    "            'mapping': {'N': 0, 'P': 1, 'Y': 2}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Street',\n",
    "            'mapping': {'Grvl': 0, 'Pave': 1}\n",
    "        },\n",
    "        {\n",
    "            'col': 'Alley',\n",
    "            'mapping': {'None': 0, 'Grvl': 1, 'Pave': 2}\n",
    "        },\n",
    "        {\n",
    "            'col': 'LandSlope',\n",
    "            'mapping': {'Gtl': 0, 'Mod': 1, 'Sev': 2}\n",
    "        },\n",
    "        {\n",
    "            'col': 'LotShape',\n",
    "            'mapping': {'Reg': 0, 'IR1': 1, 'IR2': 2, 'IR3': 3}\n",
    "        },\n",
    "        {\n",
    "            'col': 'HouseStyle', \n",
    "            'mapping': {'SLvl': 0, 'SFoyer': 0, '1Story': 1, '1.5Fin': 2, \n",
    "                        '1.5Unf': 2, '2Story': 3, '2.5Unf': 4, '2.5Fin': 4}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # list of categorical columns(23)  \n",
    "    # that we just encoded\n",
    "    ord_cat_DONE = {'FireplaceQu': str, 'GarageQual': str,'GarageCond': str,'BsmtFinType1': str,\n",
    "                     'BsmtFinType2': str,'ExterQual': str,'ExterCond': str,'BsmtQual': str,\n",
    "                     'BsmtCond': str,'PoolQC': str,'HeatingQC': str,'KitchenQual': str,\n",
    "                     'BsmtExposure': str,'Functional': str,'GarageFinish': str,'Fence': str, \n",
    "                     'CentralAir': str, 'PavedDrive': str,'Street': str,'Alley': str,\n",
    "                     'LandSlope': str,'LotShape': str, 'HouseStyle': str}\n",
    "\n",
    "    # to encode using OneHot (15 so far)\n",
    "    cat_to_1Hot = {'Condition1': str, 'RoofStyle': str, 'Foundation': str, 'GarageType': str, 'SaleType': str, \n",
    "                   'SaleCondition': str, 'Exterior1st': str, 'Exterior2nd': str, 'LotConfig': str, 'LandContour': str, \n",
    "                   'Condition2': str, 'RoofMatl': str, 'Heating': str, 'Electrical': str, 'MiscFeature': str }\n",
    "\n",
    "    # updating cat_to_1Hot with new created features\n",
    "    cat_to_1Hot.update( {'PeakMonths':str, 'Finished':str, 'Splited':str,\n",
    "                           'Neighborhood_Cluster': str, 'MSSubClass_Cluster': str} )\n",
    "\n",
    "    # updating the types\n",
    "    # update the list of ordinal with 2 columns name\n",
    "    # that allredy are ordinal and encoded ('OverallQual', 'OverallCond')\n",
    "\n",
    "    ord_cat_DONE.update({'OverallQual': str, 'OverallCond': str})#,\n",
    "                         # 'MoSold_sin': str, 'MoSold_cos': str})\n",
    "    # new_d = new_d.astype(ord_cat_DONE)\n",
    "\n",
    "    # 5 more columns will concidere as categorical\n",
    "    # we take off 'MoSold': str, \n",
    "    cat_to_1Hot.update({'YrSold': str, 'BldgType':str, \n",
    "                        'MSZoning': str, 'MasVnrType': str})\n",
    "\n",
    "    # new_d = new_d.astype(cat_to_1Hot)\n",
    "    \n",
    "    # updating the data type\n",
    "    # df = df.astype(cat_to_1Hot)\n",
    "    # df = df.astype(ord_cat_DONE)\n",
    "    \n",
    "    return (ord_cat_mapping, ord_cat_DONE, cat_to_1Hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0837ed-4ff2-4ff0-8757-22bd1ab8858e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Ordinal and OneHot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ea81b-798a-4140-b820-f079c439f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_1hot_encode(all_data, ord_mapping, oneHot_col):\n",
    "    \n",
    "    ## OrdinalEncoder\n",
    "    oe = OrdinalEncoder(mapping=ord_mapping).fit(all_data)\n",
    "    \n",
    "    all_data = oe.transform(all_data)\n",
    "    \n",
    "    \n",
    "    ## OneHot encoding\n",
    "    oh = OneHotEncoder(cols=oneHot_col).fit(all_data)\n",
    "    \n",
    "    all_data = oh.transform(all_data.copy())\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf713e-99a7-4ca7-9461-f8baf513f1fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae2292-19fb-42fe-bc1e-9f5853c623f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, verbose=False, sorted_features=False):\n",
    "    \n",
    "    mutual_info = mutual_info_regression(X=X, y=y)\n",
    "    \n",
    "    mu_info_df = pd.DataFrame(list(zip( X.columns, mutual_info )), columns=['Features', 'Mutual_info'])\n",
    "    \n",
    "    feature_to_dop = mu_info_df[ mu_info_df['Mutual_info'] == 0]\n",
    "    feature_to_dop = list( feature_to_dop['Features'] )\n",
    "    \n",
    "    if(verbose):\n",
    "        #Sorting\n",
    "        # mu_info_df.sort_values('Mutual_info', ascending=False, inplace=True)\n",
    "        print('{} Features with Zero(0) mutual info: \\n'.format(len(feature_to_dop)))\n",
    "        feature_to_dop = sorted(feature_to_dop)\n",
    "        print(feature_to_dop)\n",
    "        \n",
    "    if( (verbose == False) and sorted_features):\n",
    "        feature_to_dop = sorted(feature_to_dop)\n",
    "        \n",
    "    \n",
    "    return (feature_to_dop, mu_info_df.sort_values('Mutual_info', ascending=False).copy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaf039-97dd-424a-9acc-8e29383765df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fix skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fb205-9101-4bfc-bc98-fde5ed0cf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_skew_cols(df, cols, treshold=0.7):\n",
    "    '''\n",
    "    this function scals and fix the skewed columns\n",
    "    by applying RobustScaler() and 'yeo-johnson' transform\n",
    "    '''\n",
    "    \n",
    "    skewed = all_d[cols].skew()\n",
    "    \n",
    "    skewed = skewed[skewed >= 0.7]\n",
    "    skewed = skewed.index\n",
    "    \n",
    "    print('{} features with skewe >= {}'.format( len(skewed), treshold ) )\n",
    "    \n",
    "    fix_data_skew = Pipeline([('scaler', RobustScaler()), \n",
    "                          ('yeo', PowerTransformer(method='yeo-johnson'))])\n",
    "    \n",
    "    df[skewed] = fix_data_skew.fit_transform(df[skewed].copy())\n",
    "    \n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3532270-0c04-4774-8e29-9515299c72f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a2582-b73f-46b2-a2c6-987524757bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features(df, top_cols):\n",
    "    '''\n",
    "    function to create the interaction feature of the top_cols\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    top_cols_set = df[top_cols].copy()\n",
    "    # top_cols_set\n",
    "    \n",
    "    poly_features = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "    \n",
    "    index_backup = top_cols_set.index.copy()\n",
    "    \n",
    "    poly_set = pd.DataFrame( poly_features.fit_transform(top_cols_set.copy()), columns=poly_features.get_feature_names_out(top_cols) )\n",
    "    poly_set.set_index(index_backup, inplace=True)\n",
    "    # poly_set\n",
    "    \n",
    "    poly_set.drop(columns=top_cols, inplace=True)\n",
    "    # poly_set\n",
    "    \n",
    "    result = df.merge(poly_set, left_index=True, right_index=True)\n",
    "    \n",
    "    return result.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b9e1c-5469-49f1-a37d-97d33dc29cd0",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af68c83-8ecc-46e4-8d5b-1fc378142d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "d_tr, d_te = data(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170f1fd-2d3a-4c47-b7b8-850af2319f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking cara of some outliers\n",
    "# and making 'y' more gausian like\n",
    "d_tr = analyze_y(d_tr.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b937b56-d08f-46b3-9506-022753886a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after take care of some outlier\n",
    "# combine the training and the test\n",
    "all_d = combine_tr_te(d_tr.copy(), d_te.copy(), verbose=True)\n",
    "all_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819f332-8413-474d-8f84-fa05967cdc9b",
   "metadata": {},
   "source": [
    "### Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1aea8-67b8-46c2-9d9e-652d48a3e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing nan\n",
    "all_d = impute_nan(all_d.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c2f0a-1380-44bd-ba48-e68cfbdac267",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d.columns[all_d.isna().any()].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db751d16-b994-4e52-91af-fbe8eabe5639",
   "metadata": {},
   "source": [
    "### Reducing Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86001183-b5b7-452d-9622-49c154193ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the number of categories in some columns\n",
    "all_d = reduce_categories(all_d.copy())\n",
    "# all_d['Foundation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466796c-7055-421b-b38d-e0995b1228ec",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9dd6d-6a96-4dc8-9902-22bc5ef269aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature 'PeakMonths', 'Unfinished',\n",
    "# 'Splited', and TotalSF, and Clusters \n",
    "# transforming cyclical feature 'MoSold'\n",
    "all_d = features_engineering(all_d.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542211c3-9dcf-4e24-8e48-c286c4b409e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Identifying  \n",
    "> Categorical and Number Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647b158-d279-4add-9b25-332a7a3f5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the global variables\n",
    "ord_cat_mapping, ord_cat_DONE, cat_to_1Hot = get_global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c18f3-988c-48f9-94ef-5bd620ae359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the data type\n",
    "all_d = all_d.astype(cat_to_1Hot)\n",
    "all_d = all_d.astype(ord_cat_DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1c448-e544-4f53-8b6a-bd105ba121e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after updating all the categorical columns\n",
    "# we can now identify the number columns\n",
    "numbers_col = all_d.select_dtypes('number')\n",
    "numbers_col = numbers_col.columns.to_list()\n",
    "\n",
    "# removing the transformed cyclical feature\n",
    "# that already are float and we dont want to\n",
    "# touch them\n",
    "numbers_col.remove('MoSold_sin')\n",
    "numbers_col.remove('MoSold_cos')\n",
    "# MoSold_sin, MoSold_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710eaf2-b38f-4fc9-8083-c706286e948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_col = numbers_col + list(cat_to_1Hot.keys()) + list(ord_cat_DONE.keys())\n",
    "set( all_d.columns.to_list() ) - set( analyzed_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbe87d-f5a5-4c34-8209-a9f03eee4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set( analyzed_col ) - set( all_d.columns.to_list() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f622b3-79ed-4e28-bd28-752a0efb223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('analyzed_col len:', len(analyzed_col))\n",
    "print('all_d.columns len', len(all_d.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bee38-a49d-4939-97ca-c5004cf1d1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e22f6db8-83ed-41cc-a4e3-5b279e462120",
   "metadata": {},
   "source": [
    "### Fixing skewnnes on Number Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427948b2-3178-49dc-b207-da2386c596f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the DF returned will have numbers_col Scalered and \n",
    "# PowerTransformed\n",
    "all_d = fix_skew_cols(all_d.copy(), numbers_col) # this function keeps the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d4c33-b05d-4356-b86a-0f5744bbc59e",
   "metadata": {},
   "source": [
    "### Doing Ordinal and OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4b2ba-004b-4127-a973-9e5370fbf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding ordinal and onehot columns( keeps Id index)\n",
    "all_d = ordinal_1hot_encode(all_d.copy(), ord_cat_mapping, cat_to_1Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefa6fc-51fb-4797-8295-074b9957dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e36385-2043-4eb7-9186-a88801ad7762",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5d342-f107-449a-b20f-4faabb95a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set to work with\n",
    "d_tr = all_d[ all_d['Training'] ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde5e36-121f-4d33-9345-b40f3ad4c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get the features that has 0 mutual information with 'y'\n",
    "# and a DF with **ALL the features** with their mutual info value\n",
    "# from it we can get the top ones, to do polynomial interaction\n",
    "to_drop, mutual_info_df = feature_selection( d_tr.drop(columns='SalePrice').copy(), d_tr['SalePrice'].copy(), verbose=True, sorted_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964edab-d281-43c7-a969-a069affd9e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3d54aa-45a8-4e4f-ab9e-fca86c4f8ca7",
   "metadata": {},
   "source": [
    "### Polynomial and Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7fb1b-6b6c-491f-a3aa-52d62aa49867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mutual_info_df get the top 20 features\n",
    "# that has the hight mutual information\n",
    "# becouse it is ordered we can take the top 20\n",
    "top_20_cols = mutual_info_df.head(20)['Features'].to_list()\n",
    "# top_20_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff3d48-d9a3-402a-b93a-137ef2cac8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get the interaction factor por the top 20 \n",
    "all_d = poly_features(all_d.copy(), top_20_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cdc74-55d9-4794-af61-d592d5fd4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656327ea-db89-4f60-8a51-380ef5bbdd7e",
   "metadata": {},
   "source": [
    "> Before modeling drop the features with 0 mutual info  \n",
    "> beacous the list has 'Training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f0097-e11f-471f-8633-80122865f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep 'Training' column to identify training set in all_d\n",
    "to_drop.remove('Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceacac9-7a76-459c-8aa6-5a34c365450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d.shape, len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256c69c-1eaf-4725-84ac-ba5ed6a66403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Training' is in to_drop list\n",
    "all_d.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba43e4-a468-4a94-82d6-e994accedde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16a010-7dca-46b9-844e-956452ecbe9c",
   "metadata": {},
   "source": [
    "> Getting the training, Test set and 'y' back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454bb77-9602-4749-8d13-54f7f2a3591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tr = all_d[ all_d['Training'] ].copy()\n",
    "d_te = all_d[  ~all_d['Training'] ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19886b11-ce45-4133-9ca6-fd3dee169109",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = d_tr[ ['SalePrice'] ].copy()\n",
    "\n",
    "d_tr.drop(columns='SalePrice', inplace=True)\n",
    "d_te.drop(columns='SalePrice', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c900c-1e5a-477a-824b-27f7741c4363",
   "metadata": {},
   "source": [
    "### Outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516837dd-fd65-4906-a942-0719a413933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_outlier = LocalOutlierFactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2d9b8-9feb-47a4-a7d9-86859408175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = local_outlier.fit_predict(d_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe16fc-e43d-4fd4-88a8-e597982265c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb84bd6-01f8-4e20-9133-e503068acf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outlier = y_hat != -1\n",
    "d_tr, y = d_tr[no_outlier, : ], y[no_outlier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c81ce-049a-4e56-b2a7-8db29b7758bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54b6af-a507-4d23-8725-a4587e2f6440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccbf5b-2e4e-44ff-9485-2170164e5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[ y_hat == -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a3bef-3c1b-40c6-8451-aa492d925e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_hat[ y_hat == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee78418-ae78-4776-8a56-eb231baecfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8590e-f537-4641-a559-2e1deac96857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a123f8-9698-4cf4-b32d-d74dd0b590a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538cdaa-20af-4ed5-ad17-b331a8224b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d['Mo'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc66e5b-cc52-4ff9-bf92-4275fd469deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_d['MoSold_cos'].astype(np.floating), all_d['MoSold_sin'].astype(np.floating));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0abd9-6447-4db4-b29e-2880348b5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_d['MoSold_sin'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9b2ee-97c6-4166-9598-f5b715efd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "'MoSold_sin' in numbers_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e877e-cd69-498e-b28e-4a5a344e8fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
